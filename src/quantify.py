#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•¸ä½éŸŒæ€§é‡åŒ–æ¨¡çµ„ - å…­ç¶­åº¦éŸŒæ€§èƒ½åŠ›æ¡†æ¶
- åŸºæ–¼æ•¸ä½éŸŒæ€§ç†è«–çš„ 6 å€‹æ ¸å¿ƒèƒ½åŠ›
- æ¯å€‹èƒ½åŠ›ç”±ç¨ç«‹çš„ LLM Agent è©•åˆ†
- Absorb, Adopt, Transform, Anticipate, Rebound, Learn
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import time

try:
    from llama_cpp import Llama
except ImportError:
    raise ImportError(
        "llama-cpp-python æœªå®‰è£ã€‚è«‹åŸ·è¡Œ: uv pip install llama-cpp-python"
    )

from .utils import setup_logger, Config

logger = setup_logger(__name__)

# --------------------------------
# æ¨¡å‹é…ç½®
# --------------------------------
MODEL_PATH = Config.PROJECT_ROOT / "models" / "gpt-oss-20b-Q4_0.gguf"

DEFAULT_LLM_PARAMS = {
    "n_ctx": 49152,      # 48K context for Q4 model (conservative for 24GB GPU - 80.5% utilization, safer for large prompts)
    "n_gpu_layers": -1,
    "n_threads": 8,
    "verbose": False,
}

DEFAULT_GEN_PARAMS = {
    "temperature": 0.1,
    "max_tokens": 1500,
    "stop": ["}```", "\n\n\n"],
}

# --------------------------------
# Agent ç« ç¯€åˆ†é…é…ç½®
# --------------------------------
# æ¯å€‹ Agent åªè®€å–èˆ‡å…¶éŸŒæ€§èƒ½åŠ›ç›¸é—œçš„ç« ç¯€
# å½ˆæ€§é…ç½®ï¼šæ ¹æ“šå„ Agent çš„å¯¦éš›éœ€æ±‚è¨­å®šä¸åŒä¸Šé™

AGENT_SECTION_MAPPING = {
    "absorb": [
        "item_1a",            # é¢¨éšªå› ç´ ï¼ˆä¾›æ‡‰éˆã€ç½å®³ã€ç³»çµ±ä¸­æ–·ï¼‰- é«˜å„ªå…ˆç´š
        "item_9a",            # Controls & Procedures - é«˜å„ªå…ˆç´š
        "item_1c",            # Cybersecurity - é«˜å„ªå…ˆç´š
        "cybersecurity",      # é¡å¤–è³‡å®‰æ®µè½ï¼ˆå¦‚æœæœ‰ï¼‰
        "information_security", # Information Securityï¼ˆå¦‚æœæœ‰ï¼‰
    ],
    "adopt": [
        "item_7",             # MD&Aï¼ˆç®¡ç†å±¤å¦‚ä½•æ‡‰å°å¸‚å ´èˆ‡è¡æ“Šï¼‰- é«˜å„ªå…ˆç´š
        "item_1",             # Businessï¼ˆç­–ç•¥èˆ‡ç‡Ÿé‹æ¨¡å¼ï¼‰- ä¸­å„ªå…ˆç´š
        "item_1a",            # éƒ¨åˆ†é¢¨éšªæ‡‰å°å…§å®¹ - ä½å„ªå…ˆç´š
    ],
    "transform": [
        "item_7",             # MD&Aï¼ˆè½‰å‹è¨ˆåŠƒï¼‰- é«˜å„ªå…ˆç´š
        "item_1",             # Businessï¼ˆå•†æ¥­æ¨¡å¼è®Šé©ï¼‰- é«˜å„ªå…ˆç´š
        "esg_sustainability", # ESG/æ·¨é›¶/æ•¸ä½è½‰å‹ - ä¸­å„ªå…ˆç´š
    ],
    "anticipate": [
        "item_1a",            # é¢¨éšªè­˜åˆ¥ã€æƒ…å¢ƒèªªæ˜ - é«˜å„ªå…ˆç´š
        "item_1c",            # è³‡å®‰é¢¨éšªç›£æ¸¬ã€è©•ä¼° - é«˜å„ªå…ˆç´š
        "cybersecurity",      # å¨è„…æƒ…å ±ã€ç›£æ§ - ä¸­å„ªå…ˆç´š
        "item_9a",            # ERMã€æŒçºŒç›£æ§ - ä¸­å„ªå…ˆç´š
    ],
    "rebound": [
        "item_1c",            # Incident response, logging, escalation - é«˜å„ªå…ˆç´š
        "cybersecurity",      # äº‹ä»¶éŸ¿æ‡‰è¨ˆåŠƒ - é«˜å„ªå…ˆç´š
        "item_9a",            # Disclosure controls, remediation - ä¸­å„ªå…ˆç´š
        "item_7",             # éå»è¡æ“Šèˆ‡æ¢å¾© - ä½å„ªå…ˆç´š
    ],
    "learn": [
        "esg_sustainability", # å“¡å·¥è¨“ç·´ã€çµ„ç¹”æ–‡åŒ– - é«˜å„ªå…ˆç´š
        "item_9a",            # Internal auditã€æ”¹é€² - é«˜å„ªå…ˆç´š
        "item_1a",            # éå»ç¶“é©—èˆ‡èª¿æ•´ - ä¸­å„ªå…ˆç´š
    ],
}

# æ¯å€‹ Agent çš„æœ€å¤§ token æ•¸ï¼ˆå–® Agent åŸ·è¡Œæ¨¡å¼ - å¯¦ç”¨é…ç½®ï¼‰
# åŸºæ–¼ n_ctx=48K èˆ‡å–® Agent åŸ·è¡Œæ¨¡å¼ï¼ˆæ¯æ¬¡åªè¼‰å…¥ä¸€å€‹ Agentï¼‰ï¼š
# - Q4 æ¨¡å‹ (~11GB) + 48K context å¯¦æ¸¬ç©©å®šï¼ˆ80.5% GPU ä½¿ç”¨ç‡ï¼‰
# - å–® Agent æ¨¡å¼ï¼šæ¯å€‹ Agent åŸ·è¡Œæ™‚ç¨ç«‹è¼‰å…¥/å¸è¼‰æ¨¡å‹ï¼Œç„¡ KV cache ç´¯ç©å•é¡Œ
# - å¯¦æ¸¬ç™¼ç¾ï¼šéœ€é ç•™è¶³å¤ ç©ºé–“çµ¦ç”Ÿæˆèˆ‡ CUDA ç·©è¡
# - ä¿å®ˆè¨­å®š 45K ç‚ºä¸Šé™ï¼ˆé ç•™ 3K çµ¦ç”Ÿæˆï¼Œç¢ºä¿ç©©å®šï¼‰
#
# å„ Agent çµ±è¨ˆï¼ˆæœ€å°/å¹³å‡/æœ€å¤§ tokensï¼‰ï¼š
# - adopt:       25K / 54K / 76K â†’ è¨­å®š 45Kï¼ˆ90% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼Œå¤§å‹æ¡ˆä¾‹æå¤± 41%ï¼‰
# - learn:       18K / 33K / 69K â†’ è¨­å®š 45Kï¼ˆ95% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼Œå¤§å‹æ¡ˆä¾‹æå¤± 35%ï¼‰
# - absorb:      19K / 32K / 59K â†’ è¨­å®š 45Kï¼ˆ85% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼Œå¤§å‹æ¡ˆä¾‹æå¤± 24%ï¼‰
# - anticipate:  19K / 32K / 59K â†’ è¨­å®š 45Kï¼ˆ85% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼‰
# - transform:    8K / 29K / 54K â†’ è¨­å®š 45Kï¼ˆ95% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼Œå¤§å‹æ¡ˆä¾‹æå¤± 17%ï¼‰
# - rebound:      2K / 11K / 25K â†’ è¨­å®š 45Kï¼ˆ100% æ¡ˆä¾‹å®Œæ•´ä¿ç•™ï¼‰
MAX_TOKENS_PER_AGENT = {
    "absorb": 45000,      # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
    "adopt": 45000,       # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
    "transform": 45000,   # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
    "anticipate": 45000,  # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
    "rebound": 45000,     # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
    "learn": 45000,       # å–® Agent æ¨¡å¼ - å¤§å¹…æ¸›å°‘æˆªæ–·
}

# é è¨­å€¼ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
DEFAULT_MAX_TOKENS = 12000

# --------------------------------
# æ•¸æ“šçµæ§‹
# --------------------------------
@dataclass
class DimensionScore:
    """å–®ä¸€éŸŒæ€§èƒ½åŠ›è©•åˆ†"""
    dimension: str  # absorb, adopt, transform, anticipate, rebound, learn
    score: float  # 0-100
    confidence: int  # 0=ç¼ºä¹ä¿¡å¿ƒ, 1=é©åº¦ä¿¡å¿ƒ, 2=å¼·çƒˆä¿¡å¿ƒ
    evidence: List[str]  # å¾ 10-K é€å­—å¼•ç”¨çš„è­‰æ“š
    reasoning: str  # ç‚ºä»€éº¼é€™äº›è­‰æ“šä»£è¡¨è©²èƒ½åŠ›

@dataclass
class ResilienceScore:
    """æ•¸ä½éŸŒæ€§è©•åˆ† - å…­ç¶­åº¦æ¡†æ¶"""
    company: str
    year: int
    cik: Optional[str] = None

    # å…­å¤§éŸŒæ€§èƒ½åŠ›
    absorb: Optional[DimensionScore] = None          # å¸æ”¶è¡æ“Šèƒ½åŠ›
    adopt: Optional[DimensionScore] = None           # é©æ‡‰è¡æ“Šèƒ½åŠ›
    transform: Optional[DimensionScore] = None       # è½‰æ›è¡æ“Šèƒ½åŠ›
    anticipate: Optional[DimensionScore] = None      # é æ¸¬èƒ½åŠ›
    rebound: Optional[DimensionScore] = None         # åå½ˆèƒ½åŠ›
    learn: Optional[DimensionScore] = None           # å­¸ç¿’èƒ½åŠ›

    # æ•´é«”åˆ†æ•¸
    overall_score: float = 0.0
    overall_confidence: float = 0.0

    # å…ƒæ•¸æ“š
    agent_version: str = "1.0"
    processing_time: float = 0.0
    timestamp: str = ""

    def calculate_overall(self):
        """è¨ˆç®—æ•´é«”åˆ†æ•¸ï¼ˆ6å€‹ç¶­åº¦åŠ æ¬Šå¹³å‡ï¼‰"""
        # å¯ä»¥é¸æ“‡ç­‰æ¬Šé‡æˆ–åŠ æ¬Š
        weights = {
            "absorb": 1/6,
            "adopt": 1/6,
            "transform": 1/6,
            "anticipate": 1/6,
            "rebound": 1/6,
            "learn": 1/6,
        }

        total_score = 0.0
        total_confidence = 0.0
        count = 0

        for dim_name, weight in weights.items():
            dim_score = getattr(self, dim_name)
            if dim_score and dim_score.score is not None:
                total_score += dim_score.score * weight
                total_confidence += dim_score.confidence * weight
                count += 1

        if count > 0:
            self.overall_score = round(total_score, 2)
            # Average confidence (0-2 scale)
            self.overall_confidence = round(total_confidence, 2)

    def to_dict(self) -> Dict:
        """è½‰æ›ç‚ºå­—å…¸"""
        return asdict(self)

# --------------------------------
# LLM åŒ…è£å™¨
# --------------------------------
class LLMWrapper:
    """LLM åŒ…è£å™¨"""

    def __init__(self, model_path: Optional[Path] = None, llm_params: Optional[Dict] = None):
        self.model_path = model_path or MODEL_PATH
        self.llm_params = {**DEFAULT_LLM_PARAMS, **(llm_params or {})}
        self.llm: Optional[Llama] = None

    def load_model(self) -> bool:
        """è¼‰å…¥æ¨¡å‹"""
        if self.llm is not None:
            logger.info("æ¨¡å‹å·²è¼‰å…¥")
            return True

        try:
            logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {self.model_path}")
            logger.info(f"LLM åƒæ•¸: {self.llm_params}")
            start_time = time.time()

            self.llm = Llama(
                model_path=str(self.model_path),
                **self.llm_params
            )

            load_time = time.time() - start_time
            logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ (è€—æ™‚ {load_time:.2f}s)")
            return True

        except Exception as e:
            logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False

    def generate(self, prompt: str, override_params: Optional[Dict] = None) -> str:
        """ç”Ÿæˆå›æ‡‰"""
        if self.llm is None:
            raise RuntimeError("æ¨¡å‹å°šæœªè¼‰å…¥ï¼Œè«‹å…ˆå‘¼å« load_model()")

        params = {**DEFAULT_GEN_PARAMS, **(override_params or {})}

        try:
            response = self.llm(prompt, **params)
            return response['choices'][0]['text']
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±æ•—: {e}")
            raise

    def reset_cache(self):
        """æ¸…ç©º KV cacheï¼ˆç”¨æ–¼å¤§å‹ Agent åŸ·è¡Œå‰ï¼Œé¿å…è¨˜æ†¶é«”ç´¯ç©ï¼‰"""
        if self.llm is None:
            logger.warning("æ¨¡å‹å°šæœªè¼‰å…¥ï¼Œç„¡æ³•æ¸…ç©º cache")
            return

        try:
            self.llm.reset()
            logger.info("ğŸ”„ KV cache å·²æ¸…ç©º")
        except Exception as e:
            logger.error(f"æ¸…ç©º KV cache å¤±æ•—: {e}")

    def unload_model(self):
        """å¸è¼‰æ¨¡å‹"""
        if self.llm:
            del self.llm
            self.llm = None
        logger.info("æ¨¡å‹å·²å¸è¼‰")

# --------------------------------
# å…­å€‹éŸŒæ€§èƒ½åŠ›çš„ System Prompts
# --------------------------------

def load_prompt(name: str) -> str:
    """å¾ src/prompts è¼‰å…¥ System Prompt"""
    try:
        prompt_path = Config.PROJECT_ROOT / "src" / "prompts" / f"{name}.txt"
        if not prompt_path.exists():
            logger.error(f"Prompt æª”æ¡ˆä¸å­˜åœ¨: {prompt_path}")
            return ""
        return prompt_path.read_text(encoding="utf-8").strip()
    except Exception as e:
        logger.error(f"è¼‰å…¥ Prompt {name} å¤±æ•—: {e}")
        return ""

SYSTEM_PROMPT_ABSORB = load_prompt("absorb")

SYSTEM_PROMPT_ADOPT = load_prompt("adopt")

SYSTEM_PROMPT_TRANSFORM = load_prompt("transform")

SYSTEM_PROMPT_ANTICIPATE = load_prompt("anticipate")

SYSTEM_PROMPT_REBOUND = load_prompt("rebound")

SYSTEM_PROMPT_LEARN = load_prompt("learn")

# --------------------------------
# Helper Functions
# --------------------------------

def load_cleaned_report(company: str, year: int) -> Optional[Dict[str, str]]:
    """è¼‰å…¥å·²æ¸…ç†çš„ 10-K å ±å‘Š"""
    year_short = str(year)[-2:]

    patterns = [
        f"{company.upper()}_10-K_*-{year_short}-*_primary-document.json",
        f"{company}_10-K_*-{year_short}-*_primary-document.json",
    ]

    for pattern in patterns:
        matches = list(Config.CLEANED_DATA_DIR.glob(pattern))
        if matches:
            json_path = sorted(matches)[-1]
            logger.info(f"æ‰¾åˆ°å ±å‘Š: {json_path.name}")

            try:
                data = json.loads(json_path.read_text(encoding="utf-8"))
                return data
            except Exception as e:
                logger.error(f"è®€å–å ±å‘Šå¤±æ•— {json_path}: {e}")
                return None

    logger.warning(f"æ‰¾ä¸åˆ°å ±å‘Š: {company} {year}")
    return None

def prepare_report_context(report_data: Dict[str, str]) -> str:
    """æº–å‚™å ±å‘Šä¸Šä¸‹æ–‡ï¼ˆå·²æ£„ç”¨ - è«‹ä½¿ç”¨ extract_relevant_sectionsï¼‰"""
    sections = []

    for key, value in report_data.items():
        if key not in ["source", "company", "year", "cik"] and value:
            sections.append(f"## {key.upper()}\n{value}\n")

    return "\n".join(sections)

def extract_relevant_sections(
    report_data: Dict[str, str],
    agent_name: str,
    max_tokens: Optional[int] = None
) -> str:
    """
    ç‚ºç‰¹å®š Agent æå–ç›¸é—œç« ç¯€

    Args:
        report_data: å®Œæ•´å ±å‘Šæ•¸æ“šï¼ˆå­—å…¸ï¼‰
        agent_name: Agent åç¨± (absorb/adopt/transform/anticipate/rebound/learn)
        max_tokens: æœ€å¤§ token æ•¸é™åˆ¶ï¼ˆNone å‰‡ä½¿ç”¨è©² Agent çš„é è¨­å€¼ï¼‰

    Returns:
        çµ„åˆå¾Œçš„ç›¸é—œç« ç¯€æ–‡æœ¬ï¼ˆæ§åˆ¶åœ¨ max_tokens å…§ï¼‰
    """
    # ç²å–è©² Agent çš„ç« ç¯€åˆ—è¡¨
    sections = AGENT_SECTION_MAPPING.get(agent_name, [])

    # ç²å–è©² Agent çš„ token é™åˆ¶ï¼ˆæ”¯æ´å­—å…¸é…ç½®ï¼‰
    if max_tokens is None:
        if isinstance(MAX_TOKENS_PER_AGENT, dict):
            max_tokens = MAX_TOKENS_PER_AGENT.get(agent_name, DEFAULT_MAX_TOKENS)
        else:
            max_tokens = MAX_TOKENS_PER_AGENT

    if not sections:
        logger.warning(f"æœªæ‰¾åˆ° {agent_name} çš„ç« ç¯€æ˜ å°„ï¼Œè¿”å›ç©ºå­—ä¸²")
        return ""

    context_parts = []
    total_chars = 0
    max_chars = max_tokens * 4  # ç²—ä¼° 1 token â‰ˆ 4 chars

    for section_key in sections:
        # æª¢æŸ¥ç« ç¯€æ˜¯å¦å­˜åœ¨ä¸”éç©º
        if section_key in report_data and report_data[section_key]:
            section_text = report_data[section_key]
            section_header = f"\n\n=== {section_key.upper()} ===\n\n"

            # æª¢æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            potential_total = total_chars + len(section_header) + len(section_text)

            if potential_total > max_chars:
                # è¨ˆç®—å‰©é¤˜å¯ç”¨ç©ºé–“
                remaining_space = max_chars - total_chars - len(section_header)

                if remaining_space > 1000:  # è‡³å°‘ä¿ç•™ 1000 å­—å…ƒ
                    # æˆªæ–·ç« ç¯€
                    section_text = section_text[:remaining_space] + "\n\n[...å…§å®¹å·²æˆªæ–·]"
                    context_parts.append(section_header + section_text)
                    total_chars += len(section_header) + len(section_text)
                    logger.info(f"  {section_key}: å·²æˆªæ–·è‡³ {remaining_space} å­—å…ƒ")
                else:
                    # ç©ºé–“ä¸è¶³ï¼Œåœæ­¢æ·»åŠ 
                    logger.info(f"  {section_key}: ç©ºé–“ä¸è¶³ï¼Œè·³é")
                break

            # æ·»åŠ å®Œæ•´ç« ç¯€
            context_parts.append(section_header + section_text)
            total_chars += len(section_header) + len(section_text)
            logger.info(f"  {section_key}: {len(section_text)} å­—å…ƒ")

    # è¨˜éŒ„ç¸½è¨ˆ
    total_tokens_approx = total_chars / 4
    logger.info(f"âœ… {agent_name} ä¸Šä¸‹æ–‡: {total_chars:,} å­—å…ƒ (~{total_tokens_approx:.0f} tokens)")

    return "".join(context_parts)

def parse_json_response(response: str) -> Optional[Dict]:
    """è§£æ JSON å›æ‡‰"""
    # ç›´æ¥è§£æ
    try:
        return json.loads(response)
    except:
        pass

    # æå– JSON code block
    json_match = re.search(r'```json\s*(\\{.*?\\})\s*```', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except:
            pass

    # æå– {...}
    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(0))
        except:
            pass

    logger.error("ç„¡æ³•è§£æ JSON")
    return None

# --------------------------------
# å…­å€‹ç¨ç«‹çš„ Agent è©•åˆ†å‡½æ•¸
# --------------------------------

def agent_absorb(
    llm_wrapper: LLMWrapper,
    company: str,
    year: int,
    report_data: Dict[str, str]
) -> Optional[DimensionScore]:
    """Absorb Agent - è©•ä¼°å¸æ”¶è¡æ“Šèƒ½åŠ›ï¼ˆåªè®€å–ç›¸é—œç« ç¯€ï¼‰"""
    logger.info(f"=== Absorb Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šitem_1a, item_9a, item_1c, cybersecurity, information_security
    relevant_context = extract_relevant_sections(report_data, "absorb")

    if not relevant_context:
        logger.warning("Absorb Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_ABSORB}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the ABSORB capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(
            prompt,
            override_params={
                "temperature": 0.1,
                "max_tokens": 1500,
                "stop": ["}```", "\n\n\n"],
            }
        )

        logger.info(f"Absorb å›æ‡‰é•·åº¦: {len(response)} å­—å…ƒ")
        logger.info(f"Absorb å›æ‡‰å‰ 500 å­—: {response[:500]}")
        logger.info(f"Absorb å›æ‡‰å¾Œ 500 å­—: {response[-500:]}")

        result = parse_json_response(response)
        if not result:
            logger.error("Absorb Agent JSON è§£æå¤±æ•—")
            logger.error(f"å®Œæ•´å›æ‡‰:\n{response}")
            return None

        return DimensionScore(
            dimension="absorb",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )

    except Exception as e:
        logger.error(f"Absorb Agent å¤±æ•—: {e}")
        return None

def agent_adopt(llm_wrapper: LLMWrapper, company: str, year: int, report_data: Dict[str, str]) -> Optional[DimensionScore]:
    """Adopt Agent - è©•ä¼°é©æ‡‰è¡æ“Šèƒ½åŠ›"""
    logger.info(f"=== Adopt Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šitem_7, item_1, item_1a
    relevant_context = extract_relevant_sections(report_data, "adopt")

    if not relevant_context:
        logger.warning("Adopt Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_ADOPT}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the ADOPT capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(prompt, override_params={"temperature": 0.1, "max_tokens": 2048})
        result = parse_json_response(response)
        if not result:
            return None

        return DimensionScore(
            dimension="adopt",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )
    except Exception as e:
        logger.error(f"Adopt Agent å¤±æ•—: {e}")
        return None

def agent_transform(llm_wrapper: LLMWrapper, company: str, year: int, report_data: Dict[str, str]) -> Optional[DimensionScore]:
    """Transform Agent - è©•ä¼°è½‰æ›è¡æ“Šèƒ½åŠ›"""
    logger.info(f"=== Transform Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šitem_7, item_1, esg_sustainability
    relevant_context = extract_relevant_sections(report_data, "transform")

    if not relevant_context:
        logger.warning("Transform Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_TRANSFORM}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the TRANSFORM capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(prompt, override_params={"temperature": 0.1, "max_tokens": 2048})
        result = parse_json_response(response)
        if not result:
            return None

        return DimensionScore(
            dimension="transform",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )
    except Exception as e:
        logger.error(f"Transform Agent å¤±æ•—: {e}")
        return None

def agent_anticipate(llm_wrapper: LLMWrapper, company: str, year: int, report_data: Dict[str, str]) -> Optional[DimensionScore]:
    """Anticipate Agent - è©•ä¼°é æ¸¬èƒ½åŠ›"""
    logger.info(f"=== Anticipate Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šitem_1a, item_1c, cybersecurity, item_9a
    relevant_context = extract_relevant_sections(report_data, "anticipate")

    if not relevant_context:
        logger.warning("Anticipate Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_ANTICIPATE}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the ANTICIPATE capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(prompt, override_params={"temperature": 0.1, "max_tokens": 2048})
        result = parse_json_response(response)
        if not result:
            return None

        return DimensionScore(
            dimension="anticipate",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )
    except Exception as e:
        logger.error(f"Anticipate Agent å¤±æ•—: {e}")
        return None

def agent_rebound(llm_wrapper: LLMWrapper, company: str, year: int, report_data: Dict[str, str]) -> Optional[DimensionScore]:
    """Rebound Agent - è©•ä¼°åå½ˆèƒ½åŠ›"""
    logger.info(f"=== Rebound Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šitem_1c, cybersecurity, item_9a, item_7
    relevant_context = extract_relevant_sections(report_data, "rebound")

    if not relevant_context:
        logger.warning("Rebound Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_REBOUND}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the REBOUND capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(prompt, override_params={"temperature": 0.1, "max_tokens": 2048})
        result = parse_json_response(response)
        if not result:
            return None

        return DimensionScore(
            dimension="rebound",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )
    except Exception as e:
        logger.error(f"Rebound Agent å¤±æ•—: {e}")
        return None

def agent_learn(llm_wrapper: LLMWrapper, company: str, year: int, report_data: Dict[str, str]) -> Optional[DimensionScore]:
    """Learn Agent - è©•ä¼°å­¸ç¿’èƒ½åŠ›"""
    logger.info(f"=== Learn Agent è©•åˆ†: {company} ({year}) ===")

    # æå–ç›¸é—œç« ç¯€ï¼šesg_sustainability, item_9a, item_1a
    relevant_context = extract_relevant_sections(report_data, "learn")

    if not relevant_context:
        logger.warning("Learn Agent: ç„¡ç›¸é—œç« ç¯€ï¼Œè¿”å› None")
        return None

    prompt = f"""{SYSTEM_PROMPT_LEARN}

# Company: {company} ({year})

## 10-K Report Content (Relevant Sections):

{relevant_context}

---

Now evaluate the LEARN capability and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(prompt, override_params={"temperature": 0.1, "max_tokens": 2048})
        result = parse_json_response(response)
        if not result:
            return None

        return DimensionScore(
            dimension="learn",
            score=float(result.get("score", 0)),
            confidence=int(result.get("confidence", 0)),
            evidence=result.get("evidence", []),
            reasoning=result.get("reasoning", "")
        )
    except Exception as e:
        logger.error(f"Learn Agent å¤±æ•—: {e}")
        return None

# --------------------------------
# ä¸»è©•åˆ†å‡½æ•¸
# --------------------------------

def score_resilience(
    llm_wrapper: LLMWrapper,
    company: str,
    year: int,
    report_data: Dict[str, str]
) -> Optional[ResilienceScore]:
    """
    ä½¿ç”¨ 6 å€‹ç¨ç«‹ Agent è©•ä¼°æ•¸ä½éŸŒæ€§

    Args:
        llm_wrapper: LLM å¯¦ä¾‹ï¼ˆå·²è¼‰å…¥ï¼‰
        company: å…¬å¸åç¨±
        year: å¹´ä»½
        report_data: å ±å‘Šæ•¸æ“š

    Returns:
        ResilienceScore ç‰©ä»¶
    """
    logger.info(f"=== é–‹å§‹è©•åˆ†: {company} ({year}) ===")
    start_time = time.time()

    # åˆå§‹åŒ–çµæœ
    score_obj = ResilienceScore(
        company=company,
        year=year,
        cik=report_data.get("cik"),
        agent_version="3.0",  # å–® Agent åŸ·è¡Œæ¨¡å¼ï¼ˆå®Œå…¨éš”é›¢ï¼Œç„¡æˆªæ–·ï¼‰
        timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
    )

    # å–® Agent åŸ·è¡Œæ¨¡å¼ï¼šæ¯å€‹ Agent ç¨ç«‹è¼‰å…¥/å¸è¼‰æ¨¡å‹
    # å„ªé»ï¼š
    # 1. æ¯å€‹ Agent éƒ½èƒ½ä½¿ç”¨å®Œæ•´ 64K contextï¼ˆå¹¾ä¹ç„¡æˆªæ–·ï¼‰
    # 2. ç„¡ KV cache ç´¯ç©å•é¡Œï¼ˆæ¯æ¬¡é‡æ–°è¼‰å…¥æ¨¡å‹ï¼‰
    # 3. GPU è¨˜æ†¶é«”ç©©å®šï¼ˆä¸æœƒå› å¤šå€‹ Agent ç´¯ç©è€Œ OOMï¼‰
    #
    # æˆæœ¬ï¼š
    # - 6 æ¬¡æ¨¡å‹è¼‰å…¥/å¸è¼‰ï¼ˆæ¯æ¬¡ ~1.5sï¼Œç¸½å…± ~9sï¼‰
    # - ç¸½è™•ç†æ™‚é–“å¢åŠ ç´„ 10-20%
    #
    # Trade-off: æ™‚é–“æ›è³ªé‡ï¼ˆç„¡æˆªæ–· vs ç•¥æ…¢ï¼‰

    agent_functions = [
        ("absorb", agent_absorb),
        ("adopt", agent_adopt),
        ("transform", agent_transform),
        ("anticipate", agent_anticipate),
        ("rebound", agent_rebound),
        ("learn", agent_learn),
    ]

    for agent_name, agent_func in agent_functions:
        logger.info(f"ğŸ”„ åŸ·è¡Œ {agent_name.upper()} Agentï¼ˆå–®ç¨è¼‰å…¥æ¨¡å‹ï¼‰")

        # 1. è¼‰å…¥æ¨¡å‹ï¼ˆæ¯å€‹ Agent ç¨ç«‹è¼‰å…¥ï¼‰
        if not llm_wrapper.load_model():
            logger.error(f"âŒ {agent_name} Agent æ¨¡å‹è¼‰å…¥å¤±æ•—")
            setattr(score_obj, agent_name, None)
            continue

        # 2. åŸ·è¡Œ Agent
        try:
            result = agent_func(llm_wrapper, company, year, report_data)
            setattr(score_obj, agent_name, result)
        except Exception as e:
            logger.error(f"âŒ {agent_name} Agent åŸ·è¡Œå¤±æ•—: {e}")
            setattr(score_obj, agent_name, None)

        # 3. å¸è¼‰æ¨¡å‹ï¼ˆé‡‹æ”¾ GPU è¨˜æ†¶é«”ï¼‰
        llm_wrapper.unload_model()

    # è¨ˆç®—æ•´é«”åˆ†æ•¸
    score_obj.calculate_overall()

    # è¨˜éŒ„è™•ç†æ™‚é–“
    score_obj.processing_time = time.time() - start_time

    logger.info(f"âœ… è©•åˆ†å®Œæˆ: {score_obj.overall_score:.1f}/100 (è€—æ™‚ {score_obj.processing_time:.1f}s)")

    return score_obj

def save_score_to_file(score: ResilienceScore, output_dir: Optional[Path] = None) -> Path:
    """å„²å­˜è©•åˆ†çµæœ"""
    out_dir = output_dir or Config.SCORES_DIR
    out_dir.mkdir(parents=True, exist_ok=True)

    filename = f"{score.company}_{score.year}_score.json"
    output_path = out_dir / filename

    output_path.write_text(
        json.dumps(score.to_dict(), ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    logger.info(f"è©•åˆ†å·²å„²å­˜: {output_path}")
    return output_path

# --------------------------------
# æ¸¬è©¦å‡½æ•¸
# --------------------------------


# --------------------------------
# è©•åˆ†å“¡ Agent (Reviewer)
# --------------------------------

@dataclass
class ReviewResult:
    """è©•åˆ†å“¡å¯©æ ¸çµæœ"""
    dimension: str
    original_score: float
    original_confidence: int
    is_reasonable: bool  # è©•åˆ†æ˜¯å¦åˆç†
    suggested_adjustments: str  # å»ºè­°èª¿æ•´
    review_confidence: int  # å¯©æ ¸è€…çš„ä¿¡å¿ƒ (0-2)

SYSTEM_PROMPT_REVIEWER = """You are a **Quality Assurance Analyst** reviewing digital resilience scores.

## Your Task
Review the scoring for a specific resilience capability and determine if:
1. The score (0-100) is justified by the evidence
2. The confidence level (0-2) is appropriate
3. The reasoning is logical

## Scoring Guidelines
- **Evidence-Score Match**: Does evidence support the score?
  - Strong evidence + high score = reasonable
  - Weak evidence + high score = unreasonable
  - No evidence + low score = reasonable

- **Confidence Level Check**:
  - 0 (ç¼ºä¹): Should have empty evidence list
  - 1 (é©åº¦): Should have 2-5 pieces of evidence, some uncertainty
  - 2 (å¼·çƒˆ): Should have 4+ pieces of strong evidence, clear patterns

## Output Format (JSON ONLY):
{
  "is_reasonable": true,
  "suggested_adjustments": "Brief suggestion or 'None' if reasonable",
  "review_confidence": 2
}

### CRITICAL RULES:
- If score > 70 but evidence < 3 items â†’ is_reasonable = false
- If confidence = 2 but evidence < 4 items â†’ is_reasonable = false
- If confidence = 0 but evidence exists â†’ is_reasonable = false
- Output ONLY JSON, NO explanatory text

Start with { and end with }."""

def agent_reviewer(
    llm_wrapper: LLMWrapper,
    dimension_name: str,
    dimension_score: DimensionScore,
    report_context: str
) -> Optional[ReviewResult]:
    """è©•åˆ†å“¡ Agent - å¯©æ ¸å–®ä¸€ç¶­åº¦çš„è©•åˆ†"""
    logger.info(f"=== Reviewer Agent: {dimension_name} ===")

    prompt = f"""{SYSTEM_PROMPT_REVIEWER}

## Dimension: {dimension_name.upper()}

## Original Scoring:
- Score: {dimension_score.score}/100
- Confidence: {dimension_score.confidence} ({['ç¼ºä¹', 'é©åº¦', 'å¼·çƒˆ'][dimension_score.confidence]})
- Evidence Count: {len(dimension_score.evidence)}
- Evidence: {dimension_score.evidence[:3]}  # First 3 pieces
- Reasoning: {dimension_score.reasoning}

## Relevant Report Context (for verification):
{report_context[:5000]}

---

Review this scoring and output JSON:<|end|><|start|>assistant<|channel|>analysis<|message|><|end|><|start|>assistant<|channel|>
"""

    try:
        response = llm_wrapper.generate(
            prompt,
            override_params={
                "temperature": 0.1,
                "max_tokens": 800,
                "stop": ["}```", "\n\n\n"],
            }
        )

        result = parse_json_response(response)
        if not result:
            logger.warning(f"Reviewer Agent failed for {dimension_name}")
            return None

        return ReviewResult(
            dimension=dimension_name,
            original_score=dimension_score.score,
            original_confidence=dimension_score.confidence,
            is_reasonable=result.get("is_reasonable", True),
            suggested_adjustments=result.get("suggested_adjustments", "None"),
            review_confidence=int(result.get("review_confidence", 1))
        )

    except Exception as e:
        logger.error(f"Reviewer Agent error for {dimension_name}: {e}")
        return None


def review_all_scores(
    llm_wrapper: LLMWrapper,
    score: ResilienceScore,
    report_context: str
) -> Dict[str, ReviewResult]:
    """å¯©æ ¸æ‰€æœ‰ç¶­åº¦çš„è©•åˆ†"""
    logger.info("\n=== é–‹å§‹è©•åˆ†å“¡å¯©æ ¸ ===")
    reviews = {}

    dimensions = [
        ("absorb", score.absorb),
        ("adopt", score.adopt),
        ("transform", score.transform),
        ("anticipate", score.anticipate),
        ("rebound", score.rebound),
        ("learn", score.learn),
    ]

    for dim_name, dim_score in dimensions:
        if dim_score:
            review = agent_reviewer(llm_wrapper, dim_name, dim_score, report_context)
            if review:
                reviews[dim_name] = review

                # Log review result
                status = "âœ… åˆç†" if review.is_reasonable else "âš ï¸ éœ€èª¿æ•´"
                logger.info(f"  {dim_name}: {status} - {review.suggested_adjustments}")

    logger.info(f"\nå¯©æ ¸å®Œæˆ: {len(reviews)}/6 ç¶­åº¦")
    return reviews


def test_scoring():
    """æ¸¬è©¦è©•åˆ†ç³»çµ±"""
    company = "AAPL"
    year = 2024

    logger.info("=== æ¸¬è©¦å…­ç¶­åº¦éŸŒæ€§è©•åˆ† ===")

    # è¼‰å…¥å ±å‘Š
    report_data = load_cleaned_report(company, year)
    if not report_data:
        logger.error("å ±å‘Šè¼‰å…¥å¤±æ•—")
        return False

    # åˆå§‹åŒ– LLM wrapperï¼ˆä¸è¼‰å…¥æ¨¡å‹ï¼Œç”± score_resilience å…§éƒ¨çš„æ¯å€‹ Agent ç¨ç«‹è¼‰å…¥ï¼‰
    wrapper = LLMWrapper()

    try:
        # åŸ·è¡Œè©•åˆ†ï¼ˆå–® Agent æ¨¡å¼ï¼šæ¯å€‹ Agent ç¨ç«‹è¼‰å…¥/å¸è¼‰æ¨¡å‹ï¼‰
        score = score_resilience(wrapper, company, year, report_data)

        if score:
            # è¼‰å…¥æ¨¡å‹çµ¦è©•åˆ†å“¡å¯©æ ¸ä½¿ç”¨
            if not wrapper.load_model():
                logger.error("LLM è¼‰å…¥å¤±æ•—ï¼ˆè©•åˆ†å“¡ï¼‰")
                return False

            # åŸ·è¡Œè©•åˆ†å“¡å¯©æ ¸
            report_context = prepare_report_context(report_data)
            reviews = review_all_scores(wrapper, score, report_context)

            logger.info("\n=== è©•åˆ†çµæœ ===")
            logger.info(f"å…¬å¸: {score.company} ({score.year})")
            logger.info(f"æ•´é«”åˆ†æ•¸: {score.overall_score:.1f}/100")
            logger.info(f"æ•´é«”ä¿¡å¿ƒ: {score.overall_confidence:.2f} (å¹³å‡å€¼: 0=ç¼ºä¹, 1=é©åº¦, 2=å¼·çƒˆ)")
            logger.info(f"\nå…­ç¶­åº¦åˆ†æ•¸:")

            # Helper function to display dimension safely
            def display_dim(name_zh, name_en, dim_score):
                if dim_score:
                    conf_label = {0: "ç¼ºä¹", 1: "é©åº¦", 2: "å¼·çƒˆ"}[dim_score.confidence]
                    logger.info(f"  - {name_en} ({name_zh}): {dim_score.score:.1f} (ä¿¡å¿ƒ: {conf_label})")
                else:
                    logger.info(f"  - {name_en} ({name_zh}): N/A (è©•åˆ†å¤±æ•—)")

            display_dim("å¸æ”¶", "Absorb", score.absorb)
            display_dim("é©æ‡‰", "Adopt", score.adopt)
            display_dim("è½‰æ›", "Transform", score.transform)
            display_dim("é æ¸¬", "Anticipate", score.anticipate)
            display_dim("åå½ˆ", "Rebound", score.rebound)
            display_dim("å­¸ç¿’", "Learn", score.learn)

            # é¡¯ç¤ºå¯©æ ¸æ‘˜è¦
            if reviews:
                logger.info("\n=== è©•åˆ†å“¡å¯©æ ¸æ‘˜è¦ ===")
                reasonable_count = sum(1 for r in reviews.values() if r.is_reasonable)
                logger.info(f"ç¸½å¯©æ ¸: {len(reviews)}/6 ç¶­åº¦")
                logger.info(f"åˆç†è©•åˆ†: {reasonable_count}/{len(reviews)} ç¶­åº¦")

                # é¡¯ç¤ºéœ€èª¿æ•´çš„ç¶­åº¦
                needs_adjustment = [dim for dim, r in reviews.items() if not r.is_reasonable]
                if needs_adjustment:
                    logger.info("\nâš ï¸ éœ€èª¿æ•´ç¶­åº¦:")
                    for dim in needs_adjustment:
                        r = reviews[dim]
                        logger.info(f"  - {dim}: {r.suggested_adjustments}")
                else:
                    logger.info("\nâœ… æ‰€æœ‰ç¶­åº¦è©•åˆ†åˆç†")

            # å„²å­˜çµæœ
            output_path = save_score_to_file(score)
            logger.info(f"\nçµæœå·²å„²å­˜è‡³: {output_path}")

            logger.info("âœ… è©•åˆ†æ¸¬è©¦æˆåŠŸ")
            return True
        else:
            logger.error("è©•åˆ†å¤±æ•—")
            return False

    finally:
        wrapper.unload_model()

if __name__ == "__main__":
    test_scoring()

# --------------------------------
# å‘å¾Œç›¸å®¹å±¤ (Backward Compatibility)
# --------------------------------
def agent1_score_report(
    llm_wrapper: LLMWrapper,
    company: str,
    year: int,
    report_data: Dict[str, str]
) -> Optional[ResilienceScore]:
    """
    å‘å¾Œç›¸å®¹å‡½æ•¸ - å°æ‡‰ v1.0 çš„ agent1_score_report
    å¯¦éš›èª¿ç”¨ score_resilience
    """
    return score_resilience(llm_wrapper, company, year, report_data)

